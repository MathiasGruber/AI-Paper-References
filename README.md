# References
References for papers that I've read so that I can easily find them again. Not exhaustive, but trying to keep it up to date.

# Foundational
**2019**, Four Things Everyone Should Know to Improve Batch Normalization, [[paper](https://arxiv.org/abs/1906.03548)]
* Cecilia Summers, Michael J. Dinneen, arXiv:1906.03548 [cs.LG]

**2019**, Large Batch Optimization for Deep Learning: Training BERT in 76 minutes [[paper](https://arxiv.org/abs/1904.00962)]
* Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Cho-Jui Hsieh, 	arXiv:1904.00962 [cs.LG]

**2019**, Predicting the Generalization Gap in Deep Networks with Margin Distributions [[paper](https://arxiv.org/abs/1810.00113)]
* Yiding Jiang, Dilip Krishnan, Hossein Mobahi, Samy Bengio,	arXiv:1810.00113 [stat.ML]

**2017-2019**, Decoupled Weight Decay Regularization [[paper](https://arxiv.org/abs/1711.05101)]
* Ilya Loshchilov, Frank Hutter, 	arXiv:1711.05101 [cs.LG]

**2018**, Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks [[paper](https://arxiv.org/abs/1806.05393)] [[tensorflow-implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py)]
* Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, Jeffrey Pennington, 	arXiv:1806.05393 [stat.ML]

**2016**, All you need is a good init [[paper](https://arxiv.org/abs/1511.06422)] [[unofficial keras-code](https://github.com/ducha-aiki/LSUV-keras)]  [[pytorch-implementation](https://github.com/ducha-aiki/LSUV-pytorch)]
* Dmytro Mishkin, Jiri Matas, arXiv:1511.06422 [cs.LG]

# Traditional Machine Learning
**2019**, KTBoost: Combined Kernel and Tree Boosting [[paper](https://arxiv.org/pdf/1902.03999.pdf)] [[code](https://github.com/fabsig/KTBoost)]
* Fabio Sigrist, arXiv:1902.03999 [cs.LG]

# Style Transfer
**2018**, Learning Linear Transformations for Fast Arbitrary Style Transfer [[paper](https://arxiv.org/abs/1808.04537)] [[official pytorch-code](https://github.com/sunshineatnoon/LinearStyleTransfer)]
* Xueting Li, Sifei Liu, Jan Kautz, Ming-Hsuan Yang, arXiv:1808.04537 [cs.CV]

**2017**, Universal Style Transfer via Feature Transforms [[paper](https://arxiv.org/pdf/1705.08086.pdf)]
* Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang, arXiv:1705.08086 [cs.CV]

**2017**, Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization [[paper](https://arxiv.org/abs/1703.06868)] [[official pytorch-code](https://github.com/xunhuang1995/AdaIN-style)]
* Xun Huang, Serge Belongie, arXiv:1703.06868 [cs.CV]

# Image Classification / Object Detection
**2019**, Learning Data Augmentation Strategies for Object Detection [[paper](https://arxiv.org/abs/1906.11172v1)] 
[[code](https://github.com/tensorflow/tpu/tree/master/models/official/detection)]
* Barret Zoph, Ekin D. Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, Quoc V. Le, arXiv:1906.11172 [cs.CV]

**2019**, AutoAugment: Learning Augmentation Policies from Data [[paper](https://arxiv.org/abs/1805.09501)] [[code](https://github.com/tensorflow/models/tree/master/research/autoaugment)] [[bayesian-version in keras](https://github.com/barisozmen/deepaugment)]
* Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, Quoc V. Le, 	arXiv:1805.09501 [cs.CV]

**2019**, Fast AutoAugment [[paper](https://arxiv.org/abs/1905.00397)] [[official pytorch-code](https://github.com/kakaobrain/fast-autoaugment)]
* Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, Sungwoong Kim, arXiv:1905.00397 [cs.LG]

# Natural Language Processing
**2019**, XLNet: Generalized Autoregressive Pretraining for Language Understanding [[paper](https://arxiv.org/abs/1906.08237)] [[code](https://github.com/zihangdai/xlnet)] [[keras-implementation](https://github.com/CyberZHG/keras-xlnet)]
* Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le, 	arXiv:1906.08237 [cs.CL]

**2018**, Universal Language Model Fine-tuning for Text Classification [[paper](https://arxiv.org/abs/1801.06146)] [[implementations](https://paperswithcode.com/paper/universal-language-model-fine-tuning-for-text)]
* Jeremy Howard, Sebastian Ruder, arXiv:1801.06146 [cs.CL]

**2017**, Attention is all you need [[paper](https://arxiv.org/abs/1706.03762)] [[code](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)] [[tutorial](https://www.tensorflow.org/beta/tutorials/text/transformer)]
* Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, 	arXiv:1706.03762 [cs.CL]

# Time Series Forecasting
