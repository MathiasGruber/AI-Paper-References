# References
References for papers that I've read so that I can easily find them again. Not exhaustive, but trying to keep it up to date.

# Foundational
**2019**, Which principal components are most sensitive to distributional changes?, [[paper](https://arxiv.org/abs/1905.06318)]
* Martin Tveten, arXiv:1905.06318 [math.ST]

**2019**, Adversarial Examples Are Not Bugs, They are Features [[paper](https://arxiv.org/pdf/1905.02175.pdf)]
* Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry, arXiv:1905.02175 [stat.ML]

**2019**, Learning Loss for Active Learning [[paper](https://arxiv.org/pdf/1905.03677.pdf)]
* Donggeun Yoo, In So Kweon, arXiv:1905.03677 [cs.CV]

**2019**, Class-Balanced Loss Based on Effective Number of Samples [[paper](https://arxiv.org/abs/1901.05555)]
* Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie, 	arXiv:1901.05555 [cs.CV]

**2019**, Lookahead Optimizer: k steps forward, 1 step back, [[paper](https://arxiv.org/abs/1907.08610v1)]
* Michael R. Zhang, James Lucas, Geoffrey Hinton, Jimmy Ba, arXiv:1907.08610 [cs.LG]

**2019**, Four Things Everyone Should Know to Improve Batch Normalization, [[paper](https://arxiv.org/abs/1906.03548)]
* Cecilia Summers, Michael J. Dinneen, arXiv:1906.03548 [cs.LG]

**2019**, Large Batch Optimization for Deep Learning: Training BERT in 76 minutes [[paper](https://arxiv.org/abs/1904.00962)]
* Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Cho-Jui Hsieh, 	arXiv:1904.00962 [cs.LG]

**2019**, Predicting the Generalization Gap in Deep Networks with Margin Distributions [[paper](https://arxiv.org/abs/1810.00113)]
* Yiding Jiang, Dilip Krishnan, Hossein Mobahi, Samy Bengio,	arXiv:1810.00113 [stat.ML]

**2017-2019**, Decoupled Weight Decay Regularization [[paper](https://arxiv.org/abs/1711.05101)]
* Ilya Loshchilov, Frank Hutter, 	arXiv:1711.05101 [cs.LG]

**2018**, Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks [[paper](https://arxiv.org/abs/1806.05393)] [[tensorflow-implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py)]
* Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, Jeffrey Pennington, 	arXiv:1806.05393 [stat.ML]

**2017**, Self-Normalizing Neural Networks, [[paper](https://arxiv.org/abs/1706.02515)]
* Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter, 	arXiv:1706.02515 [cs.LG]

**2016**, All you need is a good init [[paper](https://arxiv.org/abs/1511.06422)] [[unofficial keras-code](https://github.com/ducha-aiki/LSUV-keras)]  [[pytorch-implementation](https://github.com/ducha-aiki/LSUV-pytorch)]
* Dmytro Mishkin, Jiri Matas, arXiv:1511.06422 [cs.LG]

**2009**, Measuring classifier performance: a coherent alternative to the area under the ROC curve, [[paper](https://link.springer.com/article/10.1007/s10994-009-5119-5)]
* David J. Hand, Hand, D.J. Mach Learn (2009) 77: 103.

# Traditional Machine Learning
**2019 (2012)**, High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso [[paper](https://arxiv.org/abs/1202.0515)] [[code](https://github.com/riken-aip/pyHSICLasso)]
* Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing, Masashi Sugiyama, arXiv:1202.0515 [stat.ML]

**2019**, TriMap: Large-scale Dimensionality Reduction Using Triplets, [[paper](https://arxiv.org/abs/1910.00204)] [[code](https://github.com/eamid/trimap)]
* Ehsan Amid, Manfred K. Warmuth, arXiv:1910.00204 [cs.LG]

**2019**, NGBoost: Natural Gradient Boosting for Probabilistic Prediction [[paper](https://arxiv.org/abs/1910.03225)]
* Tony Duan, Anand Avati, Daisy Yi Ding, Sanjay Basu, Andrew Y. Ng, Alejandro Schuler, arXiv:1910.03225 [cs.LG]

**2019**, KTBoost: Combined Kernel and Tree Boosting [[paper](https://arxiv.org/pdf/1902.03999.pdf)] [[code](https://github.com/fabsig/KTBoost)]
* Fabio Sigrist, arXiv:1902.03999 [cs.LG]

# Recommendation Models
**2016**, Deep Neural Networks for YouTube Recommendations, [[paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf)]
* Paul Covington, Jay Adams, Emre Sargin, Proceedings of the 10th ACM Conference on Recommender Systems, ACM, New York, NY, USA

# Probabilistic Models
**2019**, Learning Hierarchical Priors in VAEs, [[paper](https://arxiv.org/abs/1905.04982)]
* Alexej Klushyn, Nutan Chen, Richard Kurle, Botond Cseke, Patrick van der Smagt, rXiv:1905.04982 [stat.ML]

**2019**, BoTorch: Programmable Bayesian Optimization in PyTorch, [[paper](https://arxiv.org/abs/1910.06403)] [[code](https://github.com/pytorch/botorch)]
* Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, Eytan Bakshy, arXiv:1910.06403 [cs.LG]

**2019**, GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration, [[paper](https://arxiv.org/abs/1809.11165)] [[code](https://gpytorch.ai/)]
* Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger, Andrew Gordon Wilson, 	arXiv:1809.11165 [cs.LG]

**2018**, Dealing with Categorical and Integer-valued Variables in Bayesian Optimization with Gaussian Processes, [[paper](https://arxiv.org/abs/1805.03463)]
* Eduardo C. Garrido-Merchán, Daniel Hernández-Lobato,	arXiv:1805.03463 [stat.ML]

**2018**, A Tutorial on Bayesian Optimization, [[paper](https://arxiv.org/abs/1807.02811)]
* Peter I. Frazier, arXiv:1807.02811 [stat.ML]

**2018**, Subset-Conditioned Generation Using Variational Autoencoder With A Learnable Tensor-Train Induced Prior, [[paper](http://bayesiandeeplearning.org/2018/papers/55.pdf)]
* Maksim Kuznetsov et al, Third workshop on Bayesian Deep Learning (NeurIPS 2018), Montréal, Canada.

# Semi-Supervised Learning
**2020**, FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence [[paper](https://arxiv.org/abs/2001.07685)]
* Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel, arXiv:2001.07685 [cs.LG]

**2019**, MixMatch: A Holistic Approach to Semi-Supervised Learning, [[paper](https://arxiv.org/abs/1905.02249)]
* David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel, arXiv:1905.02249 [cs.LG]

# Image Genneration
**2019**, High-Fidelity Image Generation With Fewer Labels, [[paper](https://arxiv.org/abs/1903.02271v1)] [[code](https://github.com/google/compare_gan)]
* Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, Sylvain Gelly

# Style Transfer
**2019**, TraVeLGAN: Image-to-image Translation by Transformation Vector Learning [[paper](https://arxiv.org/abs/1902.09631)]
* Matthew Amodio, Smita Krishnaswamy, arXiv:1902.09631 [cs.CV]

**2018**, Learning Linear Transformations for Fast Arbitrary Style Transfer [[paper](https://arxiv.org/abs/1808.04537)] [[official pytorch-code](https://github.com/sunshineatnoon/LinearStyleTransfer)]
* Xueting Li, Sifei Liu, Jan Kautz, Ming-Hsuan Yang, arXiv:1808.04537 [cs.CV]

**2017**, Universal Style Transfer via Feature Transforms [[paper](https://arxiv.org/pdf/1705.08086.pdf)]
* Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang, arXiv:1705.08086 [cs.CV]

**2017**, Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization [[paper](https://arxiv.org/abs/1703.06868)] [[official pytorch-code](https://github.com/xunhuang1995/AdaIN-style)]
* Xun Huang, Serge Belongie, arXiv:1703.06868 [cs.CV]

# Image Classification / Object Detection
**2019**, Improving Uncertainty Estimation in Convolutional Neural Networks Using Inter-rater Agreement, [[paper](https://link.springer.com/chapter/10.1007/978-3-030-32251-9_59)]
* Martin Holm Jensen, Dan Richter Jørgensen, Raluca Jalaboi, Mads Eiler Hansen, Martin Aastrup Olsen, MICCAI 2019. MICCAI 2019. Lecture Notes in Computer Science, vol 11767. Springer, Cham

**2019**, Learning Data Augmentation Strategies for Object Detection [[paper](https://arxiv.org/abs/1906.11172v1)] 
[[code](https://github.com/tensorflow/tpu/tree/master/models/official/detection)]
* Barret Zoph, Ekin D. Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, Quoc V. Le, arXiv:1906.11172 [cs.CV]

**2019**, AutoAugment: Learning Augmentation Policies from Data [[paper](https://arxiv.org/abs/1805.09501)] [[code](https://github.com/tensorflow/models/tree/master/research/autoaugment)] [[bayesian-version in keras](https://github.com/barisozmen/deepaugment)]
* Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, Quoc V. Le, 	arXiv:1805.09501 [cs.CV]

**2019**, Fast AutoAugment [[paper](https://arxiv.org/abs/1905.00397)] [[official pytorch-code](https://github.com/kakaobrain/fast-autoaugment)]
* Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, Sungwoong Kim, arXiv:1905.00397 [cs.LG]

**2018**, Bag of Tricks for Image Classification with Convolutional Neural Networks [[paper](https://arxiv.org/abs/1812.01187)]
* Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li, arXiv:1812.01187 [cs.CV]

# Sequence Models
**2019**, Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks, [[paper](http://compneuro.uwaterloo.ca/files/publications/voelker.2019.lmu.pdf)]
* Voelker, Aaron and Kaji, Ivana and Eliasmith, Chris

# Natural Language Processing
**2019**, Patent Analytics Based on Feature Vector Space Model: A Case of IoT, [[paper](https://arxiv.org/abs/1904.08100)]
* Lei Lei, Jiaju Qi, Kan Zheng, arXiv:1904.08100 [cs.CL]

**2019**, RoBERTa: A Robustly Optimized BERT Pretraining Approach [[paper](https://arxiv.org/abs/1907.11692)] [[code](https://github.com/pytorch/fairseq/tree/master/examples/roberta)]
* Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692 [cs.CL]

**2019**, XLNet: Generalized Autoregressive Pretraining for Language Understanding [[paper](https://arxiv.org/abs/1906.08237)] [[code](https://github.com/zihangdai/xlnet)] [[keras-implementation](https://github.com/CyberZHG/keras-xlnet)]
* Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le, 	arXiv:1906.08237 [cs.CL]

**2018**, Universal Language Model Fine-tuning for Text Classification [[paper](https://arxiv.org/abs/1801.06146)] [[implementations](https://paperswithcode.com/paper/universal-language-model-fine-tuning-for-text)]
* Jeremy Howard, Sebastian Ruder, arXiv:1801.06146 [cs.CL]

**2017**, Attention is all you need [[paper](https://arxiv.org/abs/1706.03762)] [[code](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)] [[tutorial](https://www.tensorflow.org/beta/tutorials/text/transformer)]
* Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, 	arXiv:1706.03762 [cs.CL]

**2016**, Enriching Word Vectors with Subword Information [[paper](https://arxiv.org/abs/1607.04606#)]
* Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, 	arXiv:1607.04606 [cs.CL]

# Biological Representation Learning
**2019**, Probabilistic variable-length segmentation of protein sequences for discriminative motif discovery (DiMotif) and sequence embedding (ProtVecX) [[paper](https://www.nature.com/articles/s41598-019-38746-w)]
* Ehsaneddin Asgari, Alice C. McHardy & Mohammad R. K. Mofrad, Scientific Reportsvolume 9, Article number: 3577 (2019)

**2019**, Universal Deep Sequence Models for Protein Classification [[paper](https://www.biorxiv.org/content/10.1101/704874v1)]
* Nils Strodthoff, Patrick Wagner, Markus Wenzel, Wojciech Samek, doi: https://doi.org/10.1101/704874

**2019**, Unified rational protein engineering with sequence-only deep representation learning, [[paper](https://www.biorxiv.org/content/10.1101/589333v1)]
* Ethan C. Alley, Grigory Khimulya,  Surojit Biswas, Mohammed AlQuraishi, George M. Church, doi: https://doi.org/10.1101/589333

# Time Series Forecasting
**2018**, SeriesNet:A Generative Time Series Forecasting Model, [[paper](https://ieeexplore.ieee.org/document/8489522)]
* Zhipeng Shen ; Yuanming Zhang ; Jiawei Lu ; Jun Xu ; Gang Xiao, ISBN: 978-1-5090-6014-6

**2017**, Conditional Time Series Forecasting with Convolutional Neural Networks [[paper](https://arxiv.org/abs/1703.04691)]
* Anastasia Borovykh, Sander Bohte, Cornelis W. Oosterlee, arXiv:1703.04691 [stat.ML]

# Drug Design
**2019**, GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks, [[paper](https://arxiv.org/abs/2001.06216)]
* Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, Dawei Yin, Yi Chang, arXiv:2001.06216 [cs.LG]

**2019**, Evaluating Scalable Uncertainty Estimation Methods for DNN-Based Molecular Property Prediction [[paper](https://arxiv.org/abs/1910.03127)]
* Gabriele Scalia, Colin A. Grambow, Barbara Pernici, Yi-Pei Li, William H. Green, arXiv:1910.03127 [cs.LG]

**2019**, Rethinking drug design in the artificial intelligence era [[paper](https://www.nature.com/articles/s41573-019-0050-3)]
* Schneider P., Walters W., Plowright A., Sieroka N., Listgarten J., Goodnow R., Fisher J., Jansen J., Duca J., Rush T., Zentgraf M., Hill J., Krutoholow E., Kohler M., Blaney J., Funatsu K., Luebkemann C., Schneider G., Nat Rev Drug Discov (2019)

**2019**, Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules [[paper](https://arxiv.org/abs/1910.10685)]
* Benjamin Sanchez-Lengeling, Jennifer N. Wei, Brian K. Lee, Richard C. Gerkin, Alán Aspuru-Guzik, Alexander B. Wiltschko, 	arXiv:1910.10685 [stat.ML]

**2019**, Deep learning enables rapid identification of potent DDR1 kinase inhibitors [[paper](https://www.nature.com/articles/s41587-019-0224-x)]
* Alex Zhavoronkov et al., Nature Biotechnologyvolume 37, pages1038–1040 (2019)

**2019**, Deep learning for molecular design - a review of the state of the art [[paper](https://arxiv.org/abs/1903.04388)]
* Daniel C. Elton, Zois Boukouvalas, Mark D. Fuge, Peter W. Chung, 	arXiv:1903.04388 [cs.LG]

**2018**, Junction Tree Variational Autoencoder for Molecular Graph Generation [[paper](https://arxiv.org/abs/1802.04364)]
* Wengong Jin, Regina Barzilay, Tommi Jaakkola, arXiv:1802.04364 [cs.LG]

**2017**, Neural Message Passing for Quantum Chemistry, [[paper](https://arxiv.org/abs/1704.01212)] [[code](https://github.com/priba/nmp_qc)]
* Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl, arXiv:1704.01212 [cs.LG]

# Interpretation / Explainability
**2019**, Can You Trust This Prediction? Auditing Pointwise Reliability After Learning [[paper](https://arxiv.org/abs/1901.00403)]
* Peter Schulam, Suchi Saria, arXiv:1901.00403 [stat.ML]

**2017**, A Unified Approach to Interpreting Model Predictions [[paper](https://arxiv.org/abs/1705.07874)]
* Scott Lundberg, Su-In Lee, arXiv:1705.07874 [cs.AI]

# Chemistry
**2019**, Ab-Initio Solution of the Many-Electron Schrödinger Equation with Deep Neural Networks [[paper](https://arxiv.org/abs/1909.02487)]
* David Pfau, James S. Spencer, Alexander G. de G. Matthews, W. M. C. Foulkes, arXiv:1909.02487 [physics.chem-ph]

# Non-technical
**2019**, 150 successful Machine Learning models: 6 lessons learned at Booking.com, [[paper](https://www.kdd.org/kdd2019/accepted-papers/view/150-successful-machine-learning-models-6-lessons-learned-at-booking.com)]
* Pablo Estevez, Themistoklis Mavridis and Lucas Bernardi

**2018**, The state-of-the-art on Intellectual Property Analytics (IPA): A literature review on artificial intelligence, machine learning and deep learning methods for analysing intellectual property (IP) data [[paper](https://reader.elsevier.com/reader/sd/pii/S0172219018300103?token=FA15B51A34BCC633A61D1925EEC1F5922254EBB8BD769D2AAFC3596735FFA3AFA528673C2CD6FA80B238773C670A6348)]
* Aristodemou, L. and Tietze, F.
